# ====================================
# OPEA OVMS Configuration
# ====================================

# Service Endpoints
OPEA_BASE_URL=http://localhost:8888
OPEA_LLM_URL=http://localhost:9001
OPEA_EMBEDDING_URL=http://localhost:6006
OPEA_RERANK_URL=http://localhost:9201

# Milvus Configuration
MILVUS_HOST=localhost
MILVUS_PORT=19530
COLLECTION_NAME=opea_documents

# TEI Embedding Configuration
TEI_EMBEDDING_ENDPOINT=http://localhost:6006

# Model Names (must match config/*.json)
OPEA_LLM_MODEL=llama-3-8b-instruct
OPEA_EMBEDDING_MODEL=bge-m3
OPEA_RERANK_MODEL=bge-reranker-v2-m3
EMBEDDING_MODEL_ID=BAAI/bge-m3

# HuggingFace Token (for model downloads)
HF_TOKEN=your_huggingface_token_here

# Timeout Configuration (milliseconds)
OPEA_CHAT_TIMEOUT=1800000        # 30 minutes
OPEA_EMBEDDING_TIMEOUT=300000    # 5 minutes
OPEA_RERANK_TIMEOUT=300000       # 5 minutes

# Embedding Configuration
EMBEDDING_DIMENSIONS=1024

# Provider Selection
LLM_PROVIDER=opea           # Options: ionos, ollama, opea
EMBEDDING_PROVIDER=opea     # Options: ionos, ollama, opea
RERANKER_PROVIDER=opea      # Options: opea, ollama (opea recommended)

# Enable/Disable Reranking
USE_RERANKER=true

# ====================================
# OVMS Docker Configuration
# ====================================

# Proxy Settings (if needed)
http_proxy=
https_proxy=
no_proxy=localhost,127.0.0.1,milvus,tei-embedding

# GPU Device (if using GPU)
OVMS_GPU_DEVICE=0

# Log Level
OVMS_LOG_LEVEL=INFO
LOG_LEVEL=INFO

# Number of Inference Requests (nireq)
OVMS_LLM_NIREQ=4
OVMS_EMBEDDING_NIREQ=8
OVMS_RERANK_NIREQ=6

# Target Device
OVMS_TARGET_DEVICE=GPU  # Options: CPU, GPU, AUTO

# Performance Hints
OVMS_LLM_PERFORMANCE_HINT=LATENCY      # LLM: prioritize latency
OVMS_EMBEDDING_PERFORMANCE_HINT=THROUGHPUT  # Embedding: prioritize throughput
OVMS_RERANK_PERFORMANCE_HINT=THROUGHPUT     # Reranking: prioritize throughput