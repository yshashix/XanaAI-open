# Example environment configuration file
# Copy this to .env and update with your actual values

# ============================================================================
# LLM Configuration
# ============================================================================

# LLM Provider Selection
LLM_PROVIDER="ollama"
# LLM_PROVIDER options: "ollama" | "ionos" | "opea"
# - "ollama": Use local Ollama server
# - "ionos": Use IONOS cloud API
# - "opea": Use OPEA OpenVINO Model Server (OVMS)

# Ollama Configuration (when LLM_PROVIDER="ollama")
OLLAMA_BASE_URL=http://localhost:11434
LLM_MODEL="llama3.3:70b-instruct-q3_K_M"
# Ollama LLM_MODEL options: "qwen3:30b" | "llama3.3:70b-instruct-q3_K_M" | "phi4:latest"

LLM_ENDPOINT="http://localhost:11434"
LLM_API_KEY="ollama"
OLLAMA_CHAT_TIMEOUT=120000  # 120 seconds (increase for larger models)
OLLAMA_KEEP_ALIVE=30m  # Keep model loaded for 30 minutes

# OPEA OVMS Configuration (when LLM_PROVIDER="opea-ovms")
OPEA_LLM_URL=http://localhost:8000/v3/chat/completions
OPEA_LLM_MODEL=Qwen2.5-14B-Instruct-fp16-ov
# OPEA LLM_MODEL options: "Qwen2.5-14B-Instruct-fp16-ov" | "meta-llama/Llama-3.3-70B-Instruct" | "phi-3-mini-instruct"
OPEA_CHAT_TIMEOUT=1800000  # 30 minutes

# ============================================================================
# Embedding Configuration
# ============================================================================

# Embedding Provider Selection
EMBEDDING_PROVIDER="ollama"
# EMBEDDING_PROVIDER options: "ollama" | "ionos" | "opea"
# - "ollama": Use local Ollama server
# - "ionos": Use IONOS cloud API
# - "opea": Use OPEA OpenVINO Model Server (OVMS) or TEI service

EMBEDDING_ENDPOINT="http://localhost:11435/v1/embeddings"

# Ollama Embedding Configuration (when EMBEDDING_PROVIDER="ollama")
EMBEDDING_MODEL=nomic-embed-text
# Ollama EMBEDDING_MODEL options: "bge-m3:latest" | "nomic-embed-text" | "embeddinggemma:latest"

# OPEA Embedding Configuration (when EMBEDDING_PROVIDER="opea")
OPEA_EMBEDDING_URL=http://localhost:6000/v3/embeddings
OPEA_EMBEDDING_MODEL=BAAI/bge-m3
# OPEA EMBEDDING_MODEL options: "BAAI/bge-m3" | "BAAI/bge-large-en-v1.5" | "sentence-transformers/all-MiniLM-L6-v2"
OPEA_EMBEDDING_TIMEOUT=60000  # 60 seconds

EMBEDDING_DIMENSIONS=1024
HUGGINGFACE_TOKENIZER="TOKENIZER_TO_YOUR_EMBEDDING_MODEL"
RAG_EMBED_DIM=1024

# ============================================================================
# Reranking Configuration
# ============================================================================

# Reranking Provider Selection
RERANKER_PROVIDER="opea"
# RERANKER_PROVIDER options: "opea"
# - "opea": Use OPEA OpenVINO Model Server (OVMS) reranker service

USE_RERANKER=true

# OPEA Reranker Configuration (when RERANKER_PROVIDER="opea")
OPEA_RERANK_URL=http://localhost:8001/v3/rerank
OPEA_RERANK_MODEL=BAAI/bge-reranker-v2-m3
# OPEA RERANK_MODEL options: "BAAI/bge-reranker-v2-m3" | "BAAI/bge-reranker-large" | "BAAI/bge-reranker-base"
OPEA_RERANK_TIMEOUT=60000  # 60 seconds

RETRIEVE_TOP_K=5

# ============================================================================
# IONOS Inference API Configuration (when *_PROVIDER="ionos")
# ============================================================================

# IONOS API Configuration
COMPLETIONS_API_URL=https://openai.inference.de-txl.ionos.com
COMPLETIONS_API_KEY=your_ionos_api_key_here

# IONOS LLM Model (when LLM_PROVIDER="ionos")
# IONOS_LLM_MODEL="meta-llama/Llama-3.3-70B-Instruct"

# IONOS Embedding Model (when EMBEDDING_PROVIDER="ionos")
# IONOS_EMBEDDING_MODEL="Alibaba-NLP/gte-large-en-v1.5" | "BAAI/bge-m3" | "text-embedding-ada-002"

# ============================================================================
# Vector Database Configuration
# ============================================================================

# Vector Database Provider Selection
VECTOR_PROVIDER="milvus"

# Milvus Configuration
MILVUS_PROVIDER="remote"
MILVUS_RAG_PROVIDER="remote"
# MILVUS_PROVIDER options: "remote" | "local"

MILVUS_API_URL=http://your.milvus.server:19530
MILVUS_LOCAL_URL=http://localhost:19530
MILVUS_DB=default
MILVUS_TOKEN=root:Milvus

# Shared Vector Configuration
RAG_COLLECTION_NAME=custom_setup_4
RAG_EMBED_DIM=1024
RAG_EMBED_METRIC=COSINE
RAG_INGEST_DIR=./src/data/jsonld
RAG_INGEST_INTERVAL_MS=600

# ============================================================================
# Application Configuration
# ============================================================================

REGISTRY_URL=https://dev-registry.ifric.org
SECRET_KEY=your_secret_key_here
MASK_SECRET=your_mask_secret_here

# MongoDB Configuration
MONGODB_URI="mongodb://username:password@host:port/database"
MONGODB_DB=admin
MONGODB_COL=vector_store_mappings

# CORS Configuration
CORS_ORIGIN=http://localhost:3050

# PostgreSQL Configuration
PGHOST=your.postgres.host
PGPORT=5432
PGPASSWORD=your_postgres_password
PGSSL=true

# ============================================================================
# OPEA (Open Platform for Enterprise AI) Configuration
# ============================================================================

# OPEA Backend Mode
OPEA_BACKEND_MODE="ovms"
# OPEA_BACKEND_MODE options: "ovms" | "megaservice"
# - "ovms": Use OpenVINO Model Server (OVMS) direct endpoints
# - "megaservice": Use OPEA MegaService orchestration stack

# OPEA MegaService Configuration (when OPEA_BACKEND_MODE="megaservice")
OPEA_MEGASERVICE_URL=http://localhost:8888
OPEA_MEGASERVICE_TIMEOUT=300000  # 5 minutes

# ============================================================================
# Docker Compose Service Ports (Reference)
# ============================================================================
# Milvus Vector DB: 19530, 9091 (metrics)
# TEI Embedding Service: 6006
# OVMS LLM Service: 9001
# OVMS Reranker Service: 9201
# Ollama: 11434
